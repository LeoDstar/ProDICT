{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7cf54e",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b8330f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup completed\n",
      "üìÅ Output directory: /home/lestrada/tumor_type_prediction/notebooks/data/CHDM_250824_results\n",
      "üéØ Target class: ['CHDM']\n",
      "üìä Classification column: code_oncotree\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Entity Classifier Workflow - Jupyter Notebook Version\n",
    "Interactive version with cell-by-cell execution control\n",
    "\"\"\"\n",
    "\n",
    "###############\n",
    "### Imports ###\n",
    "###############\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "module_path = str(Path(\"../src/data\").resolve())\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Setup paths\n",
    "current_script_path = Path.cwd()  # Use current working directory in Jupyter\n",
    "project_root = current_script_path\n",
    "module_path = project_root / \"src\" / \"data\"\n",
    "if str(module_path) not in sys.path:\n",
    "    sys.path.append(str(module_path))\n",
    "\n",
    "# Import configuration\n",
    "from entity_model_settings_TEST import *\n",
    "\n",
    "# Create output directory\n",
    "project_root = os.path.abspath(os.getcwd())\n",
    "output_dir = os.path.join(project_root, 'data', run_folder_name)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Setup completed\")\n",
    "print(f\"üìÅ Output directory: {output_dir}\")\n",
    "print(f\"üéØ Target class: {TARGET_CLASS}\")\n",
    "print(f\"üìä Classification column: {CLASSIFIED_BY}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc6f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(entity_model_settings_TEST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b4b02",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 2: Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ddaea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 10:27:01,852 - WARNING - ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/lestrada/tumor_type_prediction/notebooks/classifier_log_20250824_102701.log' mode='a' encoding='UTF-8'> (in /tmp/ipykernel_742139/650681670.py:6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Logging configured - log file: classifier_log_20250824_102701.log\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "### Logging Setup ###\n",
    "#####################\n",
    "\n",
    "log_filename = f\"classifier_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()  # Also log to notebook output\n",
    "    ]\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"default\") \n",
    "warnings.showwarning = lambda message, category, filename, lineno, file=None, line=None: logging.warning(\n",
    "    f\"{category.__name__}: {message} (in {filename}:{lineno})\"\n",
    ")\n",
    "\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "print(f\"üìù Logging configured - log file: {log_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05116a9d",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 3: Import Custom Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0202585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 10:27:05,863 - WARNING - TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html (in /home/lestrada/miniconda3/envs/tumor_type_clasifier/lib/python3.13/site-packages/tqdm/auto.py:21)\n",
      "2025-08-24 10:27:09,197 - WARNING - ImportWarning: Tensorflow not installed; ParametricUMAP will be unavailable (in /home/lestrada/miniconda3/envs/tumor_type_clasifier/lib/python3.13/site-packages/umap/__init__.py:9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "def import_custom_modules():\n",
    "    \"\"\"Import custom modules with error handling\"\"\"\n",
    "    try:\n",
    "        import preprocessing as prep\n",
    "        import feature_selection as fs\n",
    "        import model_fit as mf\n",
    "        import graphs as grph\n",
    "        return prep, fs, mf, grph\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Error importing custom modules: {e}\")\n",
    "        print(\"Make sure the following modules are in src/data/:\")\n",
    "        print(\"- preprocessing.py\") \n",
    "        print(\"- feature_selection.py\")\n",
    "        print(\"- model_fit.py\")\n",
    "        print(\"- graphs.py\")\n",
    "        raise\n",
    "\n",
    "# Import modules\n",
    "prep, fs, mf, grph = import_custom_modules()\n",
    "print(\"‚úÖ Custom modules imported successfully\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0c264",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 4: Configuration Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98cea185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CURRENT CONFIGURATION\n",
      "================================================================================\n",
      "Target Class: ['CHDM']\n",
      "Classification Column: code_oncotree\n",
      "Data Folder: 2025.08.06_CJ_paper_final/\n",
      "Split Size: 0.3\n",
      "High Confidence Threshold: 0.7\n",
      "ElasticNet Parameters: L1_ratio=0.5, C=1\n",
      "Cross-validation: 3 splits, 67 repeats\n",
      "Nested CV: 10 tries, 3 splits\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def print_configuration():\n",
    "    \"\"\"Print current configuration settings\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CURRENT CONFIGURATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Target Class: {TARGET_CLASS}\")\n",
    "    print(f\"Classification Column: {CLASSIFIED_BY}\")\n",
    "    print(f\"Data Folder: {PROCESSED_DATA_FOLDER}\")\n",
    "    print(f\"Split Size: {SPLIT_SIZE}\")\n",
    "    print(f\"High Confidence Threshold: {HIGH_CONFIDENCE_THRESHOLD}\")\n",
    "    print(f\"ElasticNet Parameters: L1_ratio={ELNET_L1_RATIO}, C={ELNET_C_VALUE}\")\n",
    "    print(f\"Cross-validation: {ELNET_N_SPLITS} splits, {ELNET_N_REPEATS} repeats\")\n",
    "    print(f\"Nested CV: {NESTED_CV_RANDOM_STATE_TRIES} tries, {NESTED_CV_N_SPLITS} splits\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print_configuration()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d531fee8",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 5: Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "867e747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Loading data files...\n",
      "================================================================================\n",
      "üìÇ Loading intensity data from: /media/kusterlab/internal_projects/active/TOPAS/WP31/Playground/Retrospective_study/2025.08.06_CJ_paper_final/preprocessed_fp.csv\n",
      "üìÇ Loading z-scores data from: /media/kusterlab/internal_projects/active/TOPAS/WP31/Playground/Retrospective_study/2025.08.06_CJ_paper_final/full_proteome_measures_z.tsv\n",
      "üìÇ Loading metadata from: /media/kusterlab/internal_projects/active/TOPAS/WP31/Playground/LE_PROdict/paper_freeze_versions_22_08/METADATA_PANCANCER_PAPER_final.xlsx\n",
      "‚úÖ Data files loaded successfully.\n",
      "üìä Quantifications shape: (13069, 5173)\n",
      "üìä Z-scores shape: (13069, 1999)\n",
      "üìä Metadata shape: (1998, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "def load_data():\n",
    "    \"\"\"Load all required data files\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"Loading data files...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Construct file paths using configuration variables\n",
    "    intensity_path_file = FOLDER_PATH + PROCESSED_DATA_FOLDER + PREPROCESSED_FP_INTENSITY\n",
    "    z_scores_path_file = FOLDER_PATH + PROCESSED_DATA_FOLDER + PREPROCESSED_FP_Z_SCORES\n",
    "    the_metadata_file = METADATA_PATH + METADATA_FILE\n",
    "    \n",
    "    print(f\"üìÇ Loading intensity data from: {intensity_path_file}\")\n",
    "    print(f\"üìÇ Loading z-scores data from: {z_scores_path_file}\")\n",
    "    print(f\"üìÇ Loading metadata from: {the_metadata_file}\")\n",
    "    \n",
    "    try:\n",
    "        input_quantifications = prep.read_table_with_correct_sep(intensity_path_file)\n",
    "        df_z_scores = prep.read_table_with_correct_sep(z_scores_path_file)\n",
    "        input_metadata = pd.read_excel(the_metadata_file,\n",
    "                                        usecols=['Sample name', 'code_oncotree', 'Tumor cell content', 'TCC_Bioinfo', 'TCC GROUP'],\n",
    "                                        dtype={'Sample name': 'string', 'code_oncotree': 'string', 'Tumor cell content': 'float64', 'TCC_Bioinfo': 'float64', 'TCC GROUP': 'string'},\n",
    "                                        na_values=['', 'NA', 'NaN', 'nan', 'N/A', 'n/a', 'None', 'TBD', 'notavailable', 'missing'])\n",
    "\n",
    "        print(\"‚úÖ Data files loaded successfully.\")\n",
    "        print(f\"üìä Quantifications shape: {input_quantifications.shape}\")\n",
    "        print(f\"üìä Z-scores shape: {df_z_scores.shape}\")\n",
    "        print(f\"üìä Metadata shape: {input_metadata.shape}\")\n",
    "\n",
    "        return input_quantifications, df_z_scores, input_metadata\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Error loading data files: {e}\")\n",
    "        print(\"Please check that all data files exist in the specified paths:\")\n",
    "        print(f\"  - {intensity_path_file}\")\n",
    "        print(f\"  - {z_scores_path_file}\")\n",
    "        print(f\"  - {the_metadata_file}\")\n",
    "        raise\n",
    "\n",
    "# Execute data loading\n",
    "input_quantifications, df_z_scores, input_metadata = load_data()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1369e7",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 6: Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1794a9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCC GROUP\n",
       "high            885\n",
       "intermediate    633\n",
       "low             286\n",
       "very low        182\n",
       "notdefined       12\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_metadata['TCC GROUP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7dfff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Preprocessing data...\n",
      "================================================================================\n",
      "üî¨ Proteins quantifications columns: ['SYMPK', 'NUP160', 'FARP1', 'UPF1', 'IGBP1', 'PSMA1', 'COL6A2', 'TXLNA', 'POGLUT1', 'EFTUD2', 'PI4KA', 'LRCH4', 'MYH14', 'UQCRC2', 'COL6A3', 'RBM17', 'SLC25A12', 'PICALM', 'MUL1', 'ATL3', 'MMAB', 'ANKFY1', 'MYOF', 'TFG', 'TINAGL1', 'HCLS1', 'FARSA', 'CS', 'SERPINA1', 'PPM1B']\n",
      "(2586, 13069)\n",
      "üö® Proteins with empty values: ['ENPP7', 'SHOX2', 'CRYGA', 'HNRNPCL3;HNRNPCL4', 'MYBPHL']\n",
      "üß¨ Peptides binary dataframe shape: (2586, 13070)\n",
      "üìà Z-scores dataframe shape before processing: (1999, 13069)\n",
      "(1998, 13069)\n",
      "üìà Z-scores initial dataframe shape: (1998, 13073)\n",
      "‚úÖ Preprocessing completed successfully\n",
      "\n",
      "üìã Data Summary:\n",
      "Initial dataframe shape: (1998, 13073)\n",
      "Classes in dataset: {'BRCA': 238, 'CUPNOS': 112, 'CHDM': 102, 'SYNS': 84, 'LMS': 77, 'SARCNOS': 63, 'ACYC': 62, 'SFT': 52, 'MFH': 51, 'ARMS': 46, 'ES': 45, 'ACC': 39, 'IHCH': 35, 'OS': 35, 'ERMS': 29, 'DSRCT': 28, 'DDLS': 27, 'MRLS': 27, 'ULMS': 25, 'ASPS': 23, 'DIFG': 23, 'PANET': 21, 'PAAD': 21, 'MEL': 21, 'SDCA': 21, 'THYM': 19, 'COAD': 19, 'MPNST': 18, 'CHS': 16, 'LGFMS': 16, 'EPIS': 16, 'THYC': 16, 'GIST': 15, 'EHAE': 14, 'UM': 14, 'CCS': 14, 'ANGS': 13, 'READ': 13, 'LUAD': 13, 'RMS': 12, 'BA': 12, 'DFSP': 11, 'GINET': 11, 'PLEMESO': 11, 'NECNOS': 10, 'SCRMS': 10, 'MYEC': 10, 'MUCC': 9, 'ALUCA': 9, 'RCSNOS': 8, 'NSGCT': 8, 'LIPO': 8, 'EMCHS': 8, 'ESCA': 7, 'BCAC': 7, 'ANSC': 7, 'SACA': 7, 'SOC': 7, 'IMT': 7, 'PRAD': 7, 'PLRMS': 7, 'SCCNOS': 7, 'RCC': 7, 'MYCHS': 6, 'MFS': 6, 'LUNE': 6, 'ACCC': 6, 'THPD': 6, 'GINETES': 6, 'STAD': 5, 'NETNOS': 5, 'THAP': 5, 'HNSC': 5, 'UNCMAL': 5, 'VSC': 4, 'PEMESO': 4, 'PHC': 4, 'LUCA': 4, 'CEAD': 4, 'ESS': 4, 'SNA': 4, 'SNUC': 4, 'FDCS': 4, 'PECOMA': 4, 'MNET': 4, 'LCH': 4, 'NBL': 3, 'UAD': 3, 'VA': 3, 'PHCH': 3, 'PRNE': 3, 'LUACC': 3, 'AML': 3, 'FIBS': 3, 'URCA': 2, 'PADA': 2, 'CEMN': 2, 'VMM': 2, 'COADREAD': 2, 'PACT': 2, 'TT': 2, 'THHC': 2, 'SSRCC': 2, 'INTS': 2, 'NHL': 2, 'RNET': 2, 'THPA': 2, 'GRCT': 2, 'PTHC': 2, 'OCSC': 2, 'CAEXPA': 2, 'HCC': 2, 'APE': 2, 'NFIB': 2, 'CENE': 2, 'SEM': 2, 'MNG': 2, 'OOVC': 2, 'PAAC': 2, 'IFS': 2, 'SCCO': 2, 'HEMA': 2, 'PNET': 2, 'HL': 2, 'PLLS': 2, 'APAD': 2, 'ARMM': 2, 'SEF': 2, 'PRCC': 1, 'GBASC': 1, 'SGCS': 1, 'EMPD': 1, 'SBWDNET': 1, 'THME': 1, 'NST': 1, 'UPDC': 1, 'AMPCA': 1, 'SCHW': 1, 'GNBL': 1, 'SIC': 1, 'PSCC': 1, 'NMCHN': 1, 'OVT': 1, 'PCLPD': 1, 'TNET': 1, 'OYST': 1, 'GEJ': 1, 'UAS': 1, 'BCC': 1, 'MBLNOS': 1, 'OMGCT': 1, 'OGCT': 1, 'PTAD': 1, 'NUTCL': 1, 'OPHSC': 1, 'PSEC': 1, 'HDCS': 1, 'HGESS': 1, 'ALAL': 1, 'ESCC': 1, 'USARC': 1, 'UCU': 1, 'DES': 1, 'LUSC': 1, 'CHBL': 1, 'ACBC': 1, 'ASCT': 1, 'USTAD': 1, 'GBC': 1, 'SRCCR': 1, 'HNMUCM': 1, 'CESC': 1, 'POCA': 1, 'LM': 1, 'SNSC': 1, 'URMM': 1, 'LIMNET': 1, 'SEBA': 1, 'LNET': 1, 'THFO': 1, 'SCBC': 1, 'LNM': 1, 'MGCT': 1, 'OCS': 1, 'NPC': 1, 'BLSC': 1, 'MAAP': 1, 'EOV': 1, 'LXSC': 1, 'DLBCL': 1, 'TSTAD': 1, 'MPT': 1, 'PCM': 1, 'SCSRMS': 1, 'UCEC': 1}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(input_quantifications, df_z_scores, input_metadata):\n",
    "    \"\"\"Preprocess all data\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"Preprocessing data...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Protein quantification intensities post-processing\n",
    "    input_quantifications = input_quantifications.set_index(input_quantifications.columns[0])\n",
    "    peptides_quant_info = prep.post_process_meta_intensities(\n",
    "        input_quantifications.iloc[:, int(input_quantifications.shape[1]/2):].T\n",
    "    )\n",
    "    proteins_quant = input_quantifications.iloc[:, :int(input_quantifications.shape[1]/2)].T\n",
    "    print(f\"üî¨ Proteins quantifications columns: {proteins_quant.iloc[:,:10].columns.tolist()}\")\n",
    "    \n",
    "    # Imputation with configurable parameters\n",
    "    prot_quant_imputed = prep.impute_normal_down_shift_distribution(\n",
    "        proteins_quant, \n",
    "        width=IMPUTATION_WIDTH, \n",
    "        downshift=IMPUTATION_DOWNSHIFT, \n",
    "        seed=IMPUTATION_SEED\n",
    "    )\n",
    "    na_columns = prot_quant_imputed.isna().any()\n",
    "    na_columns_true = na_columns[na_columns].index.tolist()\n",
    "    print(\"üö® Proteins with empty values:\", na_columns_true)\n",
    "\n",
    "    # Cleaning sample names\n",
    "    prot_quant_imputed.reset_index(inplace=True)\n",
    "    prot_quant_imputed.rename(columns={'index': SAMPLES_COLUMN}, inplace=True)\n",
    "    prot_quant_imputed[SAMPLES_COLUMN] = prot_quant_imputed[SAMPLES_COLUMN].str.replace('pat_', '').str.strip()\n",
    "    \n",
    "    # Dataset with protein intensities and metadata\n",
    "    input_metadata['TCC'] = input_metadata['TCC_Bioinfo'].fillna(input_metadata['Tumor cell content'])\n",
    "    samples_metadata = input_metadata[[SAMPLES_COLUMN, CLASSIFIED_BY, 'TCC', 'TCC GROUP']]\n",
    "    samples_metadata[SAMPLES_COLUMN] = samples_metadata[SAMPLES_COLUMN].str.strip()\n",
    "    initial_df = samples_metadata.merge(prot_quant_imputed, on=SAMPLES_COLUMN, how='left')\n",
    "    \n",
    "    # Peptides quantification to binary dataset\n",
    "    peptides_df_binary = pd.DataFrame(\n",
    "        np.where(peptides_quant_info > 1, 1, 0),\n",
    "        index=peptides_quant_info.index,\n",
    "        columns=peptides_quant_info.columns  \n",
    "    )\n",
    "    peptides_df_binary.reset_index(inplace=True)\n",
    "    peptides_df_binary.replace('Identification metadata ', '', regex=True, inplace=True)\n",
    "    peptides_df_binary['index'] = peptides_df_binary['index'].str.strip()\n",
    "    peptides_df_binary['index'] = peptides_df_binary['index'].str.strip()\n",
    "    peptides_df_binary2 = samples_metadata.merge(peptides_df_binary, left_on=SAMPLES_COLUMN, right_on='index')\n",
    "    peptides_df_binary2.drop('index', axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"üß¨ Peptides binary dataframe shape: {peptides_df_binary.shape}\")\n",
    "    \n",
    "    # Process Z-scores\n",
    "    z_scores_df = df_z_scores.transpose(copy=True) \n",
    "    print(f\"üìà Z-scores dataframe shape before processing: {z_scores_df.shape}\")\n",
    "    z_scores_df = z_scores_df.reset_index()\n",
    "    z_scores_df = z_scores_df.replace('zscore_','', regex=True) \n",
    "    z_scores_df.rename(columns = z_scores_df.iloc[0], inplace=True)\n",
    "    z_scores_df.drop(axis=0, index=0, inplace=True)\n",
    "    z_scores_df['Gene names'] = z_scores_df.iloc[:,0].str.replace('pat_', '')\n",
    "    z_scores_df = z_scores_df.set_index('Gene names') \n",
    "    \n",
    "    z_scores_imputed = prep.impute_normal_down_shift_distribution(\n",
    "        z_scores_df,\n",
    "        width=IMPUTATION_WIDTH, \n",
    "        downshift=IMPUTATION_DOWNSHIFT, \n",
    "        seed=IMPUTATION_SEED\n",
    "    )\n",
    "    z_scores_imputed.reset_index(inplace=True)\n",
    "    z_scores_imputed.rename(columns={'Gene names': SAMPLES_COLUMN}, inplace=True)\n",
    "    z_scores_imputed[SAMPLES_COLUMN] = z_scores_imputed[SAMPLES_COLUMN].str.strip()    z_scores_imputed[SAMPLES_COLUMN] = z_scores_imputed[SAMPLES_COLUMN].str.strip()\n",
    "    z_scores_initial_df = samples_metadata.merge(z_scores_imputed, on=SAMPLES_COLUMN, how='left')\n",
    "    \n",
    "    print(f\"üìà Z-scores initial dataframe shape: {z_scores_initial_df.shape}\")\n",
    "    print(\"‚úÖ Preprocessing completed successfully\")\n",
    "    \n",
    "    return initial_df, peptides_df_binary, z_scores_initial_df, z_scores_imputed, peptides_df_binary2\n",
    "\n",
    "# Execute preprocessing\n",
    "initial_df, peptides_df_binary, z_scores_initial_df, z_scores_imputed, peptides_df_binary2 = preprocess_data(\n",
    "    input_quantifications, df_z_scores, input_metadata\n",
    ")\n",
    "\n",
    "# Display basic info about processed data\n",
    "print(f\"\\nüìã Data Summary:\")\n",
    "print(f\"Initial dataframe shape: {initial_df.shape}\")\n",
    "print(f\"Classes in dataset: {initial_df[CLASSIFIED_BY].value_counts().to_dict()}\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e8400b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560    H021-CF522-T1-Q1\n",
       "Name: Sample name, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_df['Sample name'][initial_df['Sample name'].str.contains('H021-CF522-T1-Q1')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d8f59f",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 7: Data Splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "966a329f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Splitting data...\n",
      "================================================================================\n",
      "üóëÔ∏è Removing undefined cases: ['CUPNOS', 'NETNOS', 'SARCNOS', 'SCCNOS', 'RCSNOS', 'NECNOS', 'MBLNOS', 'missing']\n",
      "Removed samples: 206\n",
      "Remaining samples: 1792\n",
      "Removed samples: 164\n",
      "Remaining samples: 1628\n",
      "Classes with only one sample: 66\n",
      "Training set samples: 1159\n",
      "Held-out set samples: 469\n",
      "Samples match between Z-score and intesntity dataset: True\n",
      "üéØ Training set size: (1159, 13073)\n",
      "üéØ Held-out set size: (469, 13073)\n",
      "üìà Z-scores training set size: (1159, 13073)\n",
      "‚úÖ Data splitting completed\n",
      "\n",
      "üìä Split Summary:\n",
      "Training classes: {'BRCA': 156, 'CHDM': 65, 'SYNS': 53, 'LMS': 50, 'ACYC': 41, 'SFT': 36, 'MFH': 32, 'ES': 32, 'ARMS': 32, 'ACC': 26, 'IHCH': 22, 'OS': 22, 'ERMS': 20, 'ULMS': 17, 'DSRCT': 17, 'DDLS': 15, 'PANET': 15, 'MRLS': 15, 'DIFG': 14, 'PAAD': 14, 'MEL': 13, 'SDCA': 13, 'ASPS': 13, 'LGFMS': 11, 'MPNST': 11, 'GIST': 11, 'CHS': 11, 'COAD': 11, 'THYC': 11, 'EPIS': 10, 'READ': 9, 'CCS': 9, 'UM': 9, 'DFSP': 8, 'BA': 8, 'RMS': 8, 'GINET': 8, 'THYM': 7, 'PLEMESO': 7, 'ANGS': 7, 'LUAD': 7, 'EHAE': 7, 'SCRMS': 7, 'MUCC': 6, 'MYEC': 6, 'NSGCT': 5, 'BCAC': 5, 'EMCHS': 5, 'SOC': 5, 'RCC': 5, 'ANSC': 5, 'PLRMS': 5, 'PRAD': 5, 'SACA': 4, 'LUNE': 4, 'THPD': 4, 'MYCHS': 4, 'ESCA': 4, 'MFS': 4, 'GINETES': 4, 'THAP': 4, 'ALUCA': 4, 'IMT': 4, 'LIPO': 4, 'ACCC': 4, 'CEAD': 3, 'HNSC': 3, 'PHC': 3, 'MNET': 3, 'ESS': 3, 'STAD': 2, 'CENE': 2, 'MNG': 2, 'NBL': 2, 'APE': 2, 'PNET': 2, 'FDCS': 2, 'VSC': 2, 'VMM': 2, 'LUACC': 2, 'CEMN': 2, 'FIBS': 2, 'PHCH': 2, 'PLLS': 2, 'NFIB': 2, 'LUCA': 2, 'PRNE': 2, 'OCSC': 2, 'SNUC': 2, 'UNCMAL': 2, 'PEMESO': 2, 'LCH': 2, 'PECOMA': 2, 'PRCC': 1, 'PADA': 1, 'EMPD': 1, 'GBASC': 1, 'URCA': 1, 'SGCS': 1, 'NHL': 1, 'OYST': 1, 'SBWDNET': 1, 'NST': 1, 'HCC': 1, 'PSCC': 1, 'SEF': 1, 'PCLPD': 1, 'GEJ': 1, 'APAD': 1, 'UAS': 1, 'NMCHN': 1, 'OVT': 1, 'TNET': 1, 'PTHC': 1, 'GNBL': 1, 'SCHW': 1, 'VA': 1, 'OOVC': 1, 'USARC': 1, 'SEM': 1, 'UCU': 1, 'TT': 1, 'UAD': 1, 'THPA': 1, 'AML': 1, 'BCC': 1, 'PAAC': 1, 'RNET': 1, 'THME': 1, 'UPDC': 1, 'HGESS': 1, 'OMGCT': 1, 'HL': 1, 'SNA': 1, 'PTAD': 1, 'OPHSC': 1, 'NUTCL': 1, 'COADREAD': 1, 'SRCCR': 1, 'PSEC': 1, 'OGCT': 1, 'HEMA': 1, 'GBC': 1, 'ASCT': 1, 'USTAD': 1, 'ACBC': 1, 'CHBL': 1, 'ALAL': 1, 'ESCC': 1, 'HNMUCM': 1, 'LUSC': 1, 'CAEXPA': 1, 'LNET': 1, 'SEBA': 1, 'CESC': 1, 'LIMNET': 1, 'URMM': 1, 'POCA': 1, 'LM': 1, 'GRCT': 1, 'PACT': 1, 'INTS': 1, 'SNSC': 1, 'SCCO': 1, 'THHC': 1, 'SCBC': 1, 'MGCT': 1, 'THFO': 1, 'OCS': 1, 'TSTAD': 1, 'DLBCL': 1, 'NPC': 1, 'MAAP': 1, 'EOV': 1, 'LXSC': 1, 'ARMM': 1, 'PCM': 1, 'MPT': 1, 'SCSRMS': 1, 'IFS': 1, 'UCEC': 1}\n",
      "Held-out classes: {'BRCA': 67, 'CHDM': 28, 'SYNS': 23, 'LMS': 22, 'ACYC': 18, 'SFT': 15, 'ES': 13, 'ARMS': 13, 'MFH': 13, 'ACC': 11, 'OS': 10, 'ERMS': 9, 'IHCH': 9, 'DSRCT': 7, 'ULMS': 7, 'MRLS': 7, 'ASPS': 6, 'DIFG': 6, 'PAAD': 6, 'PANET': 6, 'DDLS': 6, 'MEL': 6, 'SDCA': 6, 'CHS': 5, 'COAD': 5, 'MPNST': 5, 'EPIS': 4, 'CCS': 4, 'READ': 4, 'LGFMS': 4, 'UM': 4, 'THYC': 4, 'GIST': 4, 'SCRMS': 3, 'MYEC': 3, 'PLEMESO': 3, 'ANGS': 3, 'THYM': 3, 'GINET': 3, 'DFSP': 3, 'LUAD': 3, 'BA': 3, 'EHAE': 3, 'RMS': 3, 'THPD': 2, 'MYCHS': 2, 'RCC': 2, 'ANSC': 2, 'MFS': 2, 'ACCC': 2, 'ALUCA': 2, 'PRAD': 2, 'MUCC': 2, 'ESCA': 2, 'NSGCT': 2, 'LIPO': 2, 'GINETES': 2, 'PLRMS': 2, 'SOC': 2, 'EMCHS': 2, 'LUNE': 2, 'BCAC': 2, 'SACA': 1, 'GRCT': 1, 'SEF': 1, 'RNET': 1, 'COADREAD': 1, 'PADA': 1, 'INTS': 1, 'LUACC': 1, 'PAAC': 1, 'PHC': 1, 'OOVC': 1, 'APAD': 1, 'HNSC': 1, 'FDCS': 1, 'LUCA': 1, 'STAD': 1, 'PTHC': 1, 'LCH': 1, 'CAEXPA': 1, 'THAP': 1, 'UNCMAL': 1, 'URCA': 1, 'TT': 1, 'CEAD': 1, 'VA': 1, 'SNA': 1, 'AML': 1, 'ESS': 1, 'THHC': 1, 'NHL': 1, 'PRNE': 1, 'NBL': 1, 'FIBS': 1, 'VSC': 1, 'PHCH': 1, 'SNUC': 1, 'SEM': 1, 'ARMM': 1, 'PEMESO': 1, 'THPA': 1, 'IMT': 1, 'IFS': 1, 'MNET': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "def split_data(initial_df, z_scores_initial_df):\n",
    "    \"\"\"Split data into training and held-out sets\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"Splitting data...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    nos_cases = initial_df[initial_df[CLASSIFIED_BY].str.endswith('NOS', na=False)][CLASSIFIED_BY].unique().tolist()\n",
    "    cases_to_remove = nos_cases + OTHER_CASES\n",
    "    print(f\"üóëÔ∏è Removing undefined cases: {cases_to_remove}\")\n",
    "\n",
    "    # Removing samples not part of the train-validation set\n",
    "    ml_initial_df = (\n",
    "        initial_df\n",
    "        .pipe(prep.remove_class, cases_to_remove, CLASSIFIED_BY, output_dir)\n",
    "        .pipe(prep.remove_class, ['very low', 'missing'], 'TCC GROUP', output_dir)\n",
    "        .loc[lambda df: df['TCC GROUP'].notna()]\n",
    "    )\n",
    "\n",
    "    # Splitting dataset into training and held-out sets\n",
    "    training_df, held_out_df = prep.data_split(\n",
    "        ml_initial_df,\n",
    "        output_directory=output_dir, \n",
    "        split_size=SPLIT_SIZE, \n",
    "        classified_by=CLASSIFIED_BY, \n",
    "        export=True,\n",
    "    )\n",
    "    \n",
    "    # Z_scores dataset\n",
    "    z_scores_train_df = z_scores_initial_df[z_scores_initial_df['Sample name'].isin(training_df['Sample name'])]\n",
    "\n",
    "    print(f\"Samples match between Z-score and intesntity dataset: {set(training_df['Sample name']) == set(z_scores_train_df['Sample name'])}\")\n",
    "    print(f\"üéØ Training set size: {training_df.shape}\")\n",
    "    print(f\"üéØ Held-out set size: {held_out_df.shape}\")\n",
    "    print(f\"üìà Z-scores training set size: {z_scores_train_df.shape}\")\n",
    "    print(\"‚úÖ Data splitting completed\")\n",
    "    \n",
    "    return training_df, held_out_df, z_scores_train_df\n",
    "\n",
    "# Execute data splitting\n",
    "training_df, held_out_df, z_scores_train_df = split_data(initial_df, z_scores_initial_df)\n",
    "\n",
    "# Display split information\n",
    "print(f\"\\nüìä Split Summary:\")\n",
    "print(f\"Training classes: {training_df[CLASSIFIED_BY].value_counts().to_dict()}\")\n",
    "print(f\"Held-out classes: {held_out_df[CLASSIFIED_BY].value_counts().to_dict()}\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d396fae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Sample name, dtype: object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df['Sample name'][training_df['Sample name'].str.contains('H021-CF522-T1-Q1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c5a84e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Sample name, dtype: object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "held_out_df['Sample name'][held_out_df['Sample name'].str.contains('H021-CF522-T1-Q1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed26509d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCC GROUP\n",
       "high            885\n",
       "intermediate    633\n",
       "low             286\n",
       "very low        182\n",
       "notdefined       12\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_df['TCC GROUP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66e916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Initial rows: 1998\n",
      "üóëÔ∏è Removing undefined cases: ['CUPNOS', 'NETNOS', 'SARCNOS', 'SCCNOS', 'RCSNOS', 'NECNOS', 'MBLNOS', 'missing']\n",
      "Removed samples: 206\n",
      "Remaining samples: 1792\n",
      "Rows after removing cases_to_remove: 1792\n",
      "Removed samples: 164\n",
      "Remaining samples: 1628\n",
      "Rows after removing 'very low'/'missing' TCC GROUP: 1628\n",
      "Rows after dropping NA in TCC GROUP: 1618\n",
      "Classes with only one sample: 66\n",
      "Training set samples: 1152\n",
      "Held-out set samples: 466\n",
      "Rows in training set: 1152\n",
      "Rows in held-out set: 466\n",
      "Sum of splits: 1618\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_data(initial_df, z_scores_initial_df):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Initial rows: {len(initial_df)}\")\n",
    "    nos_cases = initial_df[initial_df[CLASSIFIED_BY].str.endswith('NOS', na=False)][CLASSIFIED_BY].unique().tolist()\n",
    "    cases_to_remove = nos_cases + OTHER_CASES\n",
    "    print(f\"üóëÔ∏è Removing undefined cases: {cases_to_remove}\")\n",
    "\n",
    "    df1 = prep.remove_class(initial_df, cases_to_remove, CLASSIFIED_BY, output_dir)\n",
    "    print(f\"Rows after removing cases_to_remove: {len(df1)}\")\n",
    "\n",
    "    df2 = prep.remove_class(df1, ['very low', 'notdefined'], 'TCC GROUP', output_dir)\n",
    "    print(f\"Rows after removing 'very low'/'missing' TCC GROUP: {len(df2)}\")\n",
    "\n",
    "    df3 = df2.loc[df2['TCC GROUP'].notna()]\n",
    "    print(f\"Rows after dropping NA in TCC GROUP: {len(df3)}\")\n",
    "\n",
    "    ml_initial_df = df3\n",
    "\n",
    "    training_df, held_out_df = prep.data_split(\n",
    "        ml_initial_df,\n",
    "        output_directory=output_dir, \n",
    "        split_size=SPLIT_SIZE, \n",
    "        classified_by=CLASSIFIED_BY, \n",
    "        export=False,\n",
    "    )\n",
    "    print(f\"Rows in training set: {len(training_df)}\")\n",
    "    print(f\"Rows in held-out set: {len(held_out_df)}\")\n",
    "    print(f\"Sum of splits: {len(training_df) + len(held_out_df)}\")\n",
    "    print(\"=\"*80)\n",
    "    return training_df, held_out_df, z_scores_initial_df[z_scores_initial_df['Sample name'].isin(training_df['Sample name'])]\n",
    "\n",
    "training_df, held_out_df, z_scores_train_df = split_data(initial_df, z_scores_initial_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "369b640f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Initial rows: 1998\n",
      "üóëÔ∏è Removing undefined cases: ['CUPNOS', 'NETNOS', 'SARCNOS', 'SCCNOS', 'RCSNOS', 'NECNOS', 'MBLNOS', 'missing']\n",
      "Removed samples: 206\n",
      "Remaining samples: 1792\n",
      "Rows after removing cases_to_remove: 1792\n",
      "Removed samples: 174\n",
      "Remaining samples: 1618\n",
      "Rows after removing 'very low'/'missing' TCC GROUP: 1618\n",
      "Rows after dropping NA in TCC GROUP: 1618\n",
      "Classes with only one sample: 66\n",
      "Training set samples: 1152\n",
      "Held-out set samples: 466\n",
      "Rows in training set: 1152\n",
      "Rows in held-out set: 466\n",
      "Sum of splits: 1618\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_data(initial_df, z_scores_initial_df):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Initial rows: {len(initial_df)}\")\n",
    "    nos_cases = initial_df[initial_df[CLASSIFIED_BY].str.endswith('NOS', na=False)][CLASSIFIED_BY].unique().tolist()\n",
    "    cases_to_remove = nos_cases + OTHER_CASES\n",
    "    print(f\"üóëÔ∏è Removing undefined cases: {cases_to_remove}\")\n",
    "\n",
    "    df1 = prep.remove_class(initial_df, cases_to_remove, CLASSIFIED_BY, output_dir)\n",
    "    print(f\"Rows after removing cases_to_remove: {len(df1)}\")\n",
    "\n",
    "    df2 = prep.remove_class(df1, ['very low', 'notdefined'], 'TCC GROUP', output_dir)\n",
    "    print(f\"Rows after removing 'very low'/'missing' TCC GROUP: {len(df2)}\")\n",
    "\n",
    "    df3 = df2.loc[df2['TCC GROUP'].notna()]\n",
    "    print(f\"Rows after dropping NA in TCC GROUP: {len(df3)}\")\n",
    "\n",
    "    ml_initial_df = df3\n",
    "\n",
    "    training_df, held_out_df = prep.data_split(\n",
    "        ml_initial_df,\n",
    "        output_directory=output_dir, \n",
    "        split_size=SPLIT_SIZE, \n",
    "        classified_by=CLASSIFIED_BY, \n",
    "        export=False,\n",
    "    )\n",
    "    print(f\"Rows in training set: {len(training_df)}\")\n",
    "    print(f\"Rows in held-out set: {len(held_out_df)}\")\n",
    "    print(f\"Sum of splits: {len(training_df) + len(held_out_df)}\")\n",
    "    print(\"=\"*80)\n",
    "    return training_df, held_out_df, z_scores_initial_df[z_scores_initial_df['Sample name'].isin(training_df['Sample name'])]\n",
    "\n",
    "training_df, held_out_df, z_scores_train_df = split_data(initial_df, z_scores_initial_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e9c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21e5758c",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 8: Class-Specific Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feca5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "def class_specific_workflow(training_df, held_out_df, z_scores_train_df, peptides_df_binary):\n",
    "    \"\"\"Execute class-specific workflow for specified classification\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Starting class-specific workflow for {TARGET_CLASS}...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Obtaining high confidence proteins by peptides\n",
    "    target_proteins_by_peptides = fs.get_high_confidence_proteins(\n",
    "        peptides_df_binary, TARGET_CLASS, CLASSIFIED_BY, threshold=HIGH_CONFIDENCE_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    print(f\"üî¨ Found {len(target_proteins_by_peptides)} high confidence proteins\")\n",
    "    \n",
    "    # Binary labeling for specific class classification \n",
    "    target_training_df = fs.binary_labeling(training_df, classified_by=CLASSIFIED_BY, true_class=TARGET_CLASS)\n",
    "    target_ho_df = fs.binary_labeling(held_out_df, classified_by=CLASSIFIED_BY, true_class=TARGET_CLASS)\n",
    "    target_z_scores_train_df = fs.binary_labeling(z_scores_train_df, classified_by=CLASSIFIED_BY, true_class=TARGET_CLASS)\n",
    "    \n",
    "    # 1st Filter - Filtering training and held-out dataframes by proteins with peptides\n",
    "    target_training_df = target_training_df.filter(items=[SAMPLES_COLUMN, CLASSIFIED_BY, 'Classifier'] + target_proteins_by_peptides)\n",
    "    target_ho_df = target_ho_df.filter(items=[SAMPLES_COLUMN, CLASSIFIED_BY, 'Classifier'] + target_proteins_by_peptides)\n",
    "    target_z_scores_train_df = target_z_scores_train_df.filter(items=[SAMPLES_COLUMN, CLASSIFIED_BY, 'Classifier'] + target_proteins_by_peptides)\n",
    "\n",
    "    print(f\"üìä Filtered training set shape: {target_training_df.shape}\")\n",
    "    print(f\"üìä Filtered held-out set shape: {target_ho_df.shape}\")\n",
    "    print(f\"üìà Filtered z-scores training set shape: {target_z_scores_train_df.shape}\")\n",
    "    \n",
    "    # Display class distribution\n",
    "    print(f\"\\nüéØ Binary Classification Distribution:\")\n",
    "    print(f\"Training: {target_training_df['Classifier'].value_counts().to_dict()}\")\n",
    "    print(f\"Held-out: {target_ho_df['Classifier'].value_counts().to_dict()}\")\n",
    "    \n",
    "    print(\"‚úÖ Class-specific workflow completed\")\n",
    "    \n",
    "    return target_training_df, target_ho_df, target_z_scores_train_df\n",
    "\n",
    "# Execute class-specific workflow\n",
    "target_training_df, target_ho_df, target_z_scores_train_df = class_specific_workflow(\n",
    "    training_df, held_out_df, z_scores_train_df, peptides_df_binary\n",
    ")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e21737",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_training_df['code_oncotree'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_z_scores_train_df['code_oncotree'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c5526",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 9: Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "def feature_selection_workflow(target_z_scores_train_df):\n",
    "    \"\"\"Perform feature selection using ElasticNet\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"Starting feature selection...\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üîß Using L1 ratios: {FEATURE_SELECTION_L1_RATIOS}\")\n",
    "    print(f\"üîß Using C values: {FEATURE_SELECTION_C_VALUES}\")\n",
    "    \n",
    "    # Hyperparameters for ElasticNet\n",
    "    print(\"-\"*80)\n",
    "    print(\"Defining hyperparameters for ElasticNet...\")\n",
    "\n",
    "    try:\n",
    "        target_cv_results, target_best_params, target_best_score, target_grid_search_obj = fs.hparameter_grid_search(\n",
    "            target_z_scores_train_df, GRID_SEARCH_N_SPLITS, FEATURE_SELECTION_L1_RATIOS, FEATURE_SELECTION_C_VALUES, classified_by=CLASSIFIED_BY\n",
    "        )\n",
    "        print(f\"‚úÖ Best parameters found: {target_best_params}\")\n",
    "        print(f\"üéØ Best CV score: {target_best_score:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Hyperparameter search failed: {e}\")\n",
    "        print(\"Using configured default parameters...\")\n",
    "        target_best_params = {'l1_ratio': ELNET_L1_RATIO, 'C': ELNET_C_VALUE}\n",
    "\n",
    "    # Feature Selection by ElasticNet Cross-Validation\n",
    "    print(\"-\"*80)\n",
    "    print(\"Selecting features...\")\n",
    "\n",
    "    try:\n",
    "        class_name = \"_\".join(TARGET_CLASS)\n",
    "        target_cross_val_coeffs = fs.elnet_wrapper(\n",
    "            target_z_scores_train_df, \n",
    "            classified_by=CLASSIFIED_BY, \n",
    "            tumor_type_name=f'{class_name}_features', \n",
    "            l1_ratio=target_best_params.get('l1_ratio'), \n",
    "            C=target_best_params.get('C'), \n",
    "            output_directory=output_dir,\n",
    "            n_splits=ELNET_N_SPLITS, \n",
    "            n_repeats=ELNET_N_REPEATS, \n",
    "            n_jobs=ELNET_N_JOBS, \n",
    "            export=True\n",
    "        )\n",
    "        \n",
    "        target_stats, target_proteins = fs.statistic_from_coefficients(target_cross_val_coeffs, TARGET_CLASS, output_dir)\n",
    "        print(f\"‚úÖ Selected {len(target_proteins)} protein features\")\n",
    "        print(f\"üî¨ Top 10 selected proteins: {target_proteins[:10]}\")\n",
    "        \n",
    "        return target_proteins, target_cross_val_coeffs, target_stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Feature selection failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute feature selection\n",
    "target_proteins, target_cross_val_coeffs, target_stats = feature_selection_workflow(target_z_scores_train_df)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e919ed3",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 10: Model Fitting and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "def model_fitting_workflow(target_training_df, target_ho_df, target_proteins):\n",
    "    \"\"\"Fit the final model and evaluate\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"Starting model fitting...\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üîÑ Using {NESTED_CV_RANDOM_STATE_TRIES} random state tries for nested CV\")\n",
    "    print(f\"üîÑ Using {NESTED_CV_N_SPLITS} splits for nested CV\")\n",
    "    \n",
    "    # Reshaping dataset for training and test\n",
    "    target_training_fs = fs.reshape_df_for_fitting(target_training_df, target_proteins)\n",
    "    target_test_fs = fs.reshape_df_for_fitting(target_ho_df, target_proteins)\n",
    "    \n",
    "    print(f\"üìä Training set shape after feature selection: {target_training_fs.shape}\")\n",
    "    print(f\"üìä Test set shape after feature selection: {target_test_fs.shape}\")\n",
    "\n",
    "    # Hyperparameter Selection for Logistic Regression\n",
    "    try:\n",
    "        target_nested_cv_results = mf.wrapper_nested_cv(\n",
    "            target_training_fs, \n",
    "            random_state_tries=NESTED_CV_RANDOM_STATE_TRIES, \n",
    "            n_splits=NESTED_CV_N_SPLITS, \n",
    "            classified_by=CLASSIFIED_BY\n",
    "        )\n",
    "        target_nested_hp = mf.nested_cv_hparameters_selection(target_nested_cv_results)\n",
    "        hyperparameter_C = pd.DataFrame(target_nested_hp).T.sort_values(by='count', ascending=False).index.tolist()[0]\n",
    "        print(f\"üéØ Selected hyperparameter C: {hyperparameter_C}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Hyperparameter selection failed: {e}\")\n",
    "        hyperparameter_C = 1.0  # Default value\n",
    "        print(f\"üîß Using default hyperparameter C: {hyperparameter_C}\")\n",
    "\n",
    "    # Model Fit\n",
    "    try:\n",
    "        target_log_reg_model = mf.logistic_regression_ridge(\n",
    "            target_training_fs, \n",
    "            hyperparameter_C, \n",
    "            TARGET_CLASS, \n",
    "            CLASSIFIED_BY,\n",
    "            output_dir,\n",
    "        )\n",
    "        \n",
    "        # Get results\n",
    "        target_coefficients, target_train_probabilities, target_test_probabilities = mf.logistic_regression_results(\n",
    "            target_log_reg_model, \n",
    "            target_training_fs, \n",
    "            target_test_fs,  \n",
    "            TARGET_CLASS, \n",
    "            CLASSIFIED_BY,\n",
    "            output_dir\n",
    "        )\n",
    "        \n",
    "        # Classification scores\n",
    "        test_target_scores = mf.classification_scores(target_test_probabilities)\n",
    "        \n",
    "        print(\"‚úÖ Model training and evaluation completed successfully!\")\n",
    "        print(f\"üìä Test scores summary:\")\n",
    "        if hasattr(test_target_scores, 'describe'):\n",
    "            print(test_target_scores.describe())\n",
    "        \n",
    "        return target_log_reg_model, target_coefficients, target_train_probabilities, target_test_probabilities, test_target_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during model fitting: {e}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# Execute model fitting\n",
    "model_results = model_fitting_workflow(target_training_df, target_ho_df, target_proteins)\n",
    "target_log_reg_model, target_coefficients, target_train_probabilities, target_test_probabilities, test_target_scores = model_results\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72999993",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 11: Results Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d32d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "def generate_visualizations(initial_df, test_target_scores, target_proteins):\n",
    "    \"\"\"Generate and save graphs for results exploration\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"Generating visualizations...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # UMAP plot\n",
    "        print(\"üìä Creating UMAP plot...\")\n",
    "        UMAP_plot = grph.create_umap_plot(\n",
    "            df=initial_df, \n",
    "            output_directory=output_dir,\n",
    "            feature_columns=target_proteins, \n",
    "            color_column=CLASSIFIED_BY, \n",
    "            metadata_cols=[SAMPLES_COLUMN, CLASSIFIED_BY, 'TCC GROUP'],\n",
    "            n_neighbors=5,\n",
    "        )\n",
    "        \n",
    "        # TCC vs Probability plot\n",
    "        print(\"üìà Creating TCC vs Probability plot...\")\n",
    "        TCC_plot = grph.plot_tcc_vs_probability(initial_df, test_target_scores, output_dir)\n",
    "        \n",
    "        print(\"‚úÖ Visualizations generated successfully!\")\n",
    "        return TCC_plot, UMAP_plot\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Visualization generation failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Generate visualizations (only if model was successful)\n",
    "if target_log_reg_model is not None:\n",
    "    TCC_plot, UMAP_plot = generate_visualizations(initial_df, test_target_scores, target_proteins)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping visualizations due to model fitting errors\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f68fc",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 12: Final Summary and Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4847b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "def display_final_summary():\n",
    "    \"\"\"Display final summary of the workflow\"\"\"\n",
    "    print(\"#\" * 80)\n",
    "    print(\"üèÅ FINAL WORKFLOW SUMMARY\")\n",
    "    print(\"#\" * 80)\n",
    "    \n",
    "    if target_log_reg_model is not None:\n",
    "        print(\"‚úÖ CLASSIFIER WORKFLOW COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"üéØ Target class: {TARGET_CLASS}\")\n",
    "        print(f\"üî¨ Selected {len(target_proteins)} protein features\")\n",
    "        print(f\"üìÅ Results exported to: {output_dir}\")\n",
    "        \n",
    "        # Display some key metrics if available\n",
    "        if test_target_scores is not None and not test_target_scores.empty:\n",
    "            print(f\"\\nüìä Model Performance Summary:\")\n",
    "            if 'accuracy' in test_target_scores.columns:\n",
    "                print(f\"   Average Accuracy: {test_target_scores['accuracy'].mean():.4f}\")\n",
    "            if 'roc_auc' in test_target_scores.columns:\n",
    "                print(f\"   Average ROC AUC: {test_target_scores['roc_auc'].mean():.4f}\")\n",
    "                \n",
    "        print(f\"\\nüìã Top 10 Selected Features:\")\n",
    "        print(target_proteins[:10] if len(target_proteins) >= 10 else target_proteins)\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå CLASSIFIER WORKFLOW COMPLETED WITH ERRORS\")\n",
    "        print(\"Please check the error messages in previous cells\")\n",
    "    \n",
    "    print(\"#\" * 80)\n",
    "\n",
    "# Display final summary\n",
    "display_final_summary()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9bbb2",
   "metadata": {},
   "source": [
    "\n",
    "## Cell 13: Optional - Detailed Results Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a763bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# Optional cell for detailed exploration of results\n",
    "print(\"=\"*80)\n",
    "print(\"DETAILED RESULTS EXPLORATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if target_log_reg_model is not None:\n",
    "    print(\"üìä Available results for exploration:\")\n",
    "    print(\"- target_log_reg_model: Fitted logistic regression model\")\n",
    "    print(\"- target_coefficients: Model coefficients\")\n",
    "    print(\"- target_train_probabilities: Training set predictions\")\n",
    "    print(\"- target_test_probabilities: Test set predictions\") \n",
    "    print(\"- test_target_scores: Performance metrics\")\n",
    "    print(\"- target_proteins: Selected protein features\")\n",
    "    print(\"- target_stats: Feature selection statistics\")\n",
    "    \n",
    "    # Example explorations you can run:\n",
    "    print(f\"\\nüîç Quick exploration examples:\")\n",
    "    print(f\"Model coefficients shape: {target_coefficients.shape if target_coefficients is not None else 'N/A'}\")\n",
    "    print(f\"Test probabilities shape: {target_test_probabilities.shape if target_test_probabilities is not None else 'N/A'}\")\n",
    "    \n",
    "    # Show feature importance if coefficients available\n",
    "    if target_coefficients is not None and len(target_coefficients) > 0:\n",
    "        print(f\"\\nüèÜ Top 5 most important features (by absolute coefficient):\")\n",
    "        coeff_series = pd.Series(target_coefficients, index=target_proteins)\n",
    "        top_features = coeff_series.abs().sort_values(ascending=False).head()\n",
    "        for feature, coeff in top_features.items():\n",
    "            print(f\"   {feature}: {coeff:.4f}\")\n",
    "\n",
    "print(\"\\nüí° You can now explore the results interactively in subsequent cells!\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728edc95",
   "metadata": {},
   "source": [
    "\n",
    "## Usage Instructions\n",
    "\n",
    "### How to use this notebook:\n",
    "\n",
    "1. **Run cells sequentially** - Each cell depends on the previous ones\n",
    "2. **Check outputs** - Each cell will show progress and results\n",
    "3. **Debug easily** - If a cell fails, you can fix issues and re-run just that cell\n",
    "4. **Explore results** - Use the final cells to explore your results interactively\n",
    "\n",
    "### Key variables available after execution:\n",
    "- `initial_df`: Original processed dataset\n",
    "- `training_df`, `held_out_df`: Split datasets\n",
    "- `target_proteins`: Selected protein features\n",
    "- `target_log_reg_model`: Fitted model\n",
    "- `test_target_scores`: Performance metrics\n",
    "- `target_coefficients`: Model coefficients\n",
    "\n",
    "### Benefits of this notebook structure:\n",
    "- ‚úÖ **Granular control**: Run one step at a time\n",
    "- ‚úÖ **Easy debugging**: Isolate and fix issues in specific steps\n",
    "- ‚úÖ **Interactive exploration**: Examine intermediate results\n",
    "- ‚úÖ **Flexible execution**: Skip or repeat steps as needed\n",
    "- ‚úÖ **Better visualization**: See progress and results clearly\n",
    "\n",
    "### Next steps:\n",
    "1. Save this content as `entity_classifier_notebook.ipynb`\n",
    "2. Make sure your configuration file `entity_model_settings_TEST.py` is accessible\n",
    "3. Run cells sequentially to execute your workflow\n",
    "4. Use additional cells for custom analysis and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd45d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tumor_type_clasifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
